sum(flags$orange)
flag_colors <- flags[,11:17]
head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
flag_shapes <- flags[,19:23]
lapply(flag_shapes,range)
sapply(flag_shapes,range)
shape_mat <-sapply(flag_shapes,range)
shape_mat
class(shape_mat)
unique(c(3,4,5,5,5,6,6))
unique_vals <- lapply(flags,unique)
unique_vals
sapply(unique_vals,length)
sapply(flags,unique)
lapply(unique_vals, function(elem) elem[2])
sapply(flags,unique)
vapply(flags,unique,numeric(1))
ok()
sapply(flags,class)
vapply(f;ags,class,character(1))
vapply(flags,class,character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate,flags$landmass,mean)
tapply(flags$population,flags$red,summary)
tapply(flags$population,flags$landmasses,summary)
tapply(flags$population,flags$landmass,summary)
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants,10)
tail(plants,15)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
?sample
sample(1:6,4,replace=TRUE)
sample(1:6,4,replace=TRUE)
sample(1:20,10)
LETTERS
sample(LETTERS)
flips <- sample(c(0,1),100,replace=TRUE,prob=c(0.3,0.7))
flips
sum(flips)
?rbinom
rbinom(1,size=100,prob=0.7)
flips2 <- rbinom(1,size=100,prob=0.7)
flips2 <- rbinom(n=100,size=1,prob=0.7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(10,100,25)
?rpois
rpois(5,10)
my_pois <- replicate(100,rpois(5,10))
my_pois
cm <- colMeans(my_pois)
hist(cm)
d1 <- Sys.Date()
class(d1)
unclass(d1)
d1
d2 <- as.Date("1969-01-01")
unclass(d2)
t1 <- Sys.time()
t1
class(t1)
unclass(t1)
t2 <- as.POSIXlt(Sys.time())
class(t2)
t2
unclass(t2)
str(unclass(t2))
t2$min
weekdays(t1)
weekdays(d1)
weekdays(t1)
months(t1)
quarters(t2)
ts <- "October 17, 1986 08:24"
t3 <- "October 17, 1986 08:24"
strptime(t3,"%B %d, %y %H:%M")
strptime(t3,"%B %d, %Y %H:%M")
t4 <-strptime(t3,"%B %d, %Y %H:%M")
t4
class(t4)
Sys.time() > t1
Sys.time()-t1
difftime(Sys.time(),t1,units='days')
data(cars)
?cars
head(cars)
plot(cars)
?plot
plot(x=cars$speed,y=cars$dist)
plot(x=cars$dist,y=cars$speed)
?plot
plot(x=cars$dist,y=cars$speed,xlab="Speed")
plot(x=cars$dist,y=cars$speed,ylab="Speed")
plot(x=cars$speed,y=cars$dist,xlab="Speed")
plot(x=cars$speed,y=cars$dist,ylab="Stopping Distance")
plot(x=cars$speed,y=cars$dist,ylab="Stopping Distance",xlab="Speed")
?plot
plot(x=cars$speed,y=cars$dist,ylab="Stopping Distance",xlab="Speed",main="My Plot")
plot(cars,main="My plot")
plot(cars,main="My Plot")
plot(cars,sub="My Plot Subtitle")
?par
plot(cars,col=2)
plot(cars,xlim=c(10,15))
plot(cars,pch=2)
load(mtcars)
data(mtcars)
?boxplot
boxplopt(mtcars,formula=mpg~cyl)
boxplot(mtcars,formula=mpg~cyl)
boxplot(formula = mpg ~ cyl, data = mtcars)
hist(mtcars$mpg)
install_packages(KernSmooth)
install.packages(KernSmooth)
install.packages(KernSmooth R)
install.packages(KernSmooth)
install.packages("KernSmooth")
library(KernSmooth)
cube <- function(x,n){x^3}
cube(3)
x <- 1:10
if(x>5){x <- 0}
f<-function(x){g<-function(y){y+z};z<-4;x+g(x)}
z<-10
f(3)
x<-5
y<-if(x<3){NA}else{10}
y
x<-matrix(c(-1,1),c(-1,1),2,2)
?matrix
a<-c(-1,1)
b<-c(-1,1)
m<-matrix(c(a,b),2,2)
m
a<-c(1,1)
b<-c(-1,-1)
m<-matrix(c(a,b),2,2)
m
solve(m)
?solve
test
test()
makeCacheMatrix <- function(x = matrix()) {
## @x: a square invertible matrix
## return: a list containing functions to
##              1. set the matrix
##              2. get the matrix
##              3. set the inverse
##              4. get the inverse
##         this list is used as the input to cacheSolve()
inv = NULL
set = function(y) {
# use `<<-` to assign a value to an object in an environment
# different from the current environment.
x <<- y
inv <<- NULL
}
get = function() x
setinv = function(inverse) inv <<- inverse
getinv = function() inv
list(set=set, get=get, setinv=setinv, getinv=getinv)
}
cacheSolve <- function(x, ...) {
## @x: output of makeCacheMatrix()
## return: inverse of the original matrix input to makeCacheMatrix()
inv = x$getinv()
# if the inverse has already been calculated
if (!is.null(inv)){
# get it from the cache and skips the computation.
message("getting cached data")
return(inv)
}
# otherwise, calculates the inverse
mat.data = x$get()
inv = solve(mat.data, ...)
# sets the value of the inverse in the cache via the setinv function.
x$setinv(inv)
return(inv)
}
test = function(mat){
## @mat: an invertible matrix
temp = makeCacheMatrix(mat)
start.time = Sys.time()
cacheSolve(temp)
dur = Sys.time() - start.time
print(dur)
start.time = Sys.time()
cacheSolve(temp)
dur = Sys.time() - start.time
print(dur)
}
test()
mymatrice <- matrix(rnorm(100),10,10)
mymatrice
test(mymatrice)
test(mymatrice)
install.packages("devtools")
devtools::install_github("gmlang/ezplot")
library(ezplot)
plt = mk_boxplot(films)
title1 = "Annual Distribution of Budget from 1913 to 2014"
p = plt("year_cat", "budget", ylab="budget", main=title1)
print(p)
scale_axis(p, "y", use_comma=T)
scale_axis(p, "y", use_log=T)
scale_axis(p, "y", use_log10=T)
?ezplot
set.seed(1)
rpois(5,2)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv",destfile="quizdata",method="curl")
quizcsv <- read.csv("quizdata")
colnames(quizcsv)
class(quizcsv)
millplus <- quizcsv$val == 24
millplus
?count
count(quizcsv, "val")
count(quizcsv, vars="val")
library("plyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
count(quizcsv, vars="val")
count(quizcsv, vars="VAL")
myfes <- quizcsv$FES
myfes
?read.excel
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx",destfile="quizxl.xlsx",method="curl")
library(xlsx)
?read.xlsx
library(xlxs)
install.packages("xlsx")
library(xlxs)
library("xlsx", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
rowindex <- 18:23
colindex <- 7:15
dat <- read.xlsx("quizxl.xlsx",sheetIndex=1,colIndex=colindex,rowIndex=rowindex)
sum(dat$Zip*dat$Ext,na.rm=T)
library(xml)
library(XML)
install.package("XML")
install.packages("XML")
library(XML)
doc <- xmlTreeParse("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml",useInternal=TRUE)
doc <- xmlTreeParse("https://d396qusza40orc.cloudfront.net/getdata/data/restaurants.xml",useInternal=TRUE)
?xmlTreeParse
doc <- xmlTreeParse("http://d396qusza40orc.cloudfront.net/getdata/data/restaurants.xml",useInternal=TRUE)
names(doc)
names(rootNode)
rootNode <- xmlRoot(doc)
names(rootNode)
names(row)
xmlName(rootNode)
rootNode
node[@zipcode='21231']
node[zipcode='21231']
rootNode[[1]]
rootNode[[1][1]]
rootNode[[1]][[1]]
xpathSApply(rootNode,"//zipcode",xmlValue)
xpathSApply(rootNode,"//zipcode",xmlValue='21231')
?xpathSApply
zip <- xpathSApply(doc, "/response/row/row/zipcode", xmlValue)
sum(zip == "21231")
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv",destfile="idaho.csv",method="curl")
?fread
?fRead
install.packages(data.table)
install.packages("data.table")
libray(data.table)
library(data.table)
?fread
DT <- fread("idaho.csv")
head(DT)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time((mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)))
system.time(mean(DT$pwgtp15,by=DT$SEX))
mean(DT$pwgtp15,by=DT$SEX)
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
str(BodyWeight)
Str(Diet)
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
qplot(Wind, Ozone, data = airquality, geom = "smooth")
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
Cor(x,w)
library("UsingR", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
cor
cor(x,w)
mean(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
plot(y~x)
lm(y ~ 0 + x)
data(mtcars)
lm(mtcars$mpg ~ mtcars$wt)
.5*.5
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x - mean(x)) / sd(x)
scale(x)[1]
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mu <- c(0.573, 0.8, 0.36, 0.44)
for (i in 1:length(mu)){
mse <- sum((x - mu[i])^2)
print(mse)
}
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
for (i in c(
.147,   # 3.717  => minimum
.0025,  # 3.863
.3,     # 3.880
1.077)  # 9.769
) { print(sum(w*((x-i)^2))) }
lm(w ~ x + I(x^2))
lm(w ~ x)
lm(w ~ 0+x)
pbeta(.75,1,1)
qbeta(.5,1,1)
qbeta(.5,2,1)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
sum(predictors)
summary(predictors)
summary(diagnosis)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
?hist
hist(SuperPlasticizer)
hist(concrete$SuperPlasticizer)
hist(concrete)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
summary(training)
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
install.packages("e1071")
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confustion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with the caret package
modelFit <- train(training$diagnosis ~ ., method = "glm", preProcess = "pca",
data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
setwd("~/Downloads/DataProductsProject")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
install.packages("ISLR")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
?plot
shiny::runApp()
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
coeff(fit)
coef(fit)
summary(coef(fit))
summary(fit)$coefficients
data(mtcars)
fit <- lm(mpg ~ wt, data = mtcars)
newdata <- data.frame(wt=mean(mtcars$wt))
predict(fit, newdata, interval = ("confidence"))[2]
data(mtcars)
fit <- lm(mpg ~ wt, data = mtcars)
summary(fit)
?mtcars
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
yhat <- beta0 + beta1 * x
e <- y - yhat        # residuals
sigma <- sqrt(sum(e^2) / (n - 2))
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
pBeta1
sigma
fit <- lm(mpg ~ wt, data = mtcars)
predict(fit, data.frame(wt = 3), interval="prediction")
fit <- lm(mpg ~ I(wt / 2), data = mtcars)
coef <- summary(fit)$coefficients
mean <- coef[2,1]
stderr <- coef[2,2]
df <- fit$df
#Two sides T-Tests
mean + c(-1,1) * qt(0.975, df = df) * stderr
y <- mtcars$mpg; x <- mtcars$wt
fitWithIntercept <- lm(y ~ x)
yhat1 <- fit$coefficients[1] + x
se1 <- sum((y - yhat1)^2)
yhat2 <- fit$coefficients[1] + fit$coefficients[2] * x
se2 <- sum((y - yhat2)^2)
ratio <- se2 / se1
ratio
data(mtcars)
lm(y ~ offset(x))
fit <- lm(mpg ~ wt, mtcars)
anova(fit)
(847.73)/(847.73 + 278.32)
data(mtcars)
# fit a linear regression model of weight (predictor) on mpg (outcome).
# Get a 95% confidence interval for the expected mpg at the average weight.
# What is the lower endpoint?
wt <- mtcars$wt
mpt <- mtcars$mpg
fit <- lm(mpg ~ wt, mtcars)
newdata <- data.frame(wt = mean(wt))
x <- predict(fit, newdata, interval = ("confidence"))
x
fit <- lm(mpg ~ wt, mtcars)
fit2 <- lm(mpg ~ offset(wt), mtcars)
a <- summary(fit)
a
pnorm(70,mean=80,sd=10)
qnorm(.95,mean=1100,sd=75)
?qnorm
rnorm(100,mean=1100,sd=75)
pnorm(2.8)
?pnorm
ppois(10,lambda=5*3)
